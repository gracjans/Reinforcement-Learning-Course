{
 "cells": [
  {
   "metadata": {
    "id": "EfLmopJAoHZn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# Laboratorium 7\n",
    "\n",
    "Celem siódmego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"
   ]
  },
  {
   "metadata": {
    "id": "Wel4AaGnoHZq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Dołączenie standardowych bibliotek"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "id": "aJePndlGoHZq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "id": "EnIxa2nKoHZs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Dołączenie bibliotek do obsługi sieci neuronowych"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "id": "vDOqSrqWoHZs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.math import log\n",
    "from tensorflow.math import reduce_sum\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "id": "Qm81TXjsoHZt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Zadanie 1 - Actor-Critic\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu należy utworzyć dwie głębokie sieci neuronowe:\n",
    "    1. *actor* - sieć, która będzie uczyła się optymalnej strategii (podobna do tej z laboratorium 6),\n",
    "    2. *critic* - sieć, która będzie uczyła się funkcji oceny stanu (podobnie jak się DQN).\n",
    "Wagi sieci *actor* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n",
    "\\end{equation*}\n",
    "Wagi sieci *critic* aktualizowane są zgodnie ze wzorem:\n",
    "\\begin{equation*}\n",
    "    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n",
    "\\end{equation*}\n",
    "gdzie:\n",
    "\\begin{equation*}\n",
    "    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n",
    "\\end{equation*}\n",
    "</p>"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "id": "utH2ckRNoHZt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, action_size, policy_model, actor, critic):\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        self.action_space = [i for i in range(action_size)]\n",
    "        self.policy_model = policy_model\n",
    "        self.actor = actor\n",
    "        self.critic = critic #critic network should have only one output'\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, basing on policy returned by the network.\n",
    "\n",
    "        Note: To pick action according to the probability generated by the network\n",
    "        \"\"\"\n",
    "        #\n",
    "        # INSERT CODE HERE to get action in a given state\n",
    "        #        \n",
    "        state = state[np.newaxis, :]\n",
    "        chosen_action = np.random.choice(self.action_space, p=self.policy_model.predict(state)[0])   \n",
    "\n",
    "        return chosen_action\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Function learn networks using information about state, action, reward and next state. \n",
    "        First the values for state and next_state should be estimated based on output of critic network.\n",
    "        Critic network should be trained based on target value:\n",
    "        target = r + \\gamma next_state_value if not done]\n",
    "        target = r if done.\n",
    "        Actor network shpuld be trained based on delta value:\n",
    "        delta = target - state_value\n",
    "        \"\"\"\n",
    "        #\n",
    "        # INSERT CODE HERE to train network\n",
    "        #\n",
    "        state = state[np.newaxis, :]\n",
    "        next_state = next_state[np.newaxis, :]\n",
    "        critic_value_next_state = self.critic.predict(next_state)\n",
    "        critic_value = self.critic.predict(state)\n",
    "\n",
    "        target = reward + self.gamma * critic_value_next_state * (1 - int(done))\n",
    "        delta = target - critic_value\n",
    "\n",
    "        actions = np.zeros([1, self.action_size])\n",
    "        actions[np.arange(1), action] = 1\n",
    "\n",
    "        self.actor.fit([state, delta], actions, verbose=0)\n",
    "        self.critic.fit(state, target, verbose=0)        "
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "id": "PWVr_VeToHZu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIcP0NHZoHZv",
    "outputId": "4895e1f6-dcba-4bd7-e9e6-c94d2e103beb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "alpha_learning_rate = 0.0001\n",
    "beta_learning_rate = 0.0005\n",
    "\n",
    "input = Input(shape=(state_size,))\n",
    "delta = Input(shape=[1])\n",
    "x1 = Dense(64, activation='relu')(input)\n",
    "probs = Dense(action_size, activation='softmax')(x1)\n",
    "values = Dense(1, activation='linear')(x1)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    log_lik = y_true * log(y_pred)\n",
    "    return reduce_sum(-log_lik * delta)\n",
    "\n",
    "policy_model = Model(inputs=[input], outputs=[probs])\n",
    "\n",
    "#\n",
    "# INSERT CODE HERE to build actor network, in the last layer use softmax activation function\n",
    "#\n",
    "actor_model = Model(inputs=[input, delta], outputs=[probs])\n",
    "actor_model.compile(optimizer=Adam(lr=alpha_learning_rate), loss=custom_loss)\n",
    "\n",
    "#\n",
    "# INSERT CODE HERE to build critic network, in the last layer use linear activation function, network should have single output\n",
    "#\n",
    "critic_model = Model(inputs=[input], outputs=[values])\n",
    "critic_model.compile(optimizer=Adam(lr=beta_learning_rate), loss='mean_squared_error')\n"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "id": "lDqYNzOzoHZv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Czas nauczyć agenta gry w środowisku *CartPool*:"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWhjwoeIoHZw",
    "outputId": "e22e8de8-ef36-4c99-c2f7-7d32aa034760",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = ActorCriticAgent(action_size, policy_model, actor_model, critic_model)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    score_history = []\n",
    "\n",
    "    for i in range(100):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        score_history.append(score)\n",
    "\n",
    "    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n",
    "\n",
    "    if np.mean(score_history) > 300:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean reward:18.540\n",
      "mean reward:20.160\n",
      "mean reward:21.520\n",
      "mean reward:31.130\n",
      "mean reward:43.740\n",
      "mean reward:51.200\n",
      "mean reward:66.480\n",
      "mean reward:135.130\n",
      "mean reward:155.290\n",
      "mean reward:207.310\n",
      "mean reward:131.190\n",
      "mean reward:138.320\n",
      "mean reward:165.600\n",
      "mean reward:119.780\n",
      "mean reward:168.280\n",
      "mean reward:104.000\n",
      "mean reward:138.560\n",
      "mean reward:124.490\n",
      "mean reward:160.210\n",
      "mean reward:157.270\n",
      "mean reward:165.310\n",
      "mean reward:534.780\n",
      "You Win!\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "name": "Lab07-Gracjan_Strzelec.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}