{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 1 (1 pkt)\n",
    "\n",
    "Celem pierwszego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmów uczenia pasywnego. Zaimplementowane algorytmy będą testowane z wykorzystaniem wcześniej przygotowanego środowiska przedstawionego na schemacie poniżej:\n",
    "![MDP, Markov Decision Process](assets/images/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dołączenie biblioteki ze środowiskiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.simpleMDP import simpleMDP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapoznanie się z przygotowanym środowiskiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s0', 's1', 's2']\n",
      "\n",
      "State: s0 action: a0 list of possible next states:  {'s0': 0.5, 's2': 0.5}\n",
      "from state: s0, action: a0, next state: s0, propablity: 0.5, reward: 0.0\n",
      "from state: s0, action: a0, next state: s2, propablity: 0.5, reward: 0.0\n",
      "\n",
      "State: s0 action: a1 list of possible next states:  {'s2': 1}\n",
      "from state: s0, action: a1, next state: s2, propablity: 1, reward: 0.0\n",
      "\n",
      "State: s1 action: a0 list of possible next states:  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "from state: s1, action: a0, next state: s0, propablity: 0.7, reward: 5\n",
      "from state: s1, action: a0, next state: s1, propablity: 0.1, reward: 0.0\n",
      "from state: s1, action: a0, next state: s2, propablity: 0.2, reward: 0.0\n",
      "\n",
      "State: s1 action: a1 list of possible next states:  {'s1': 0.95, 's2': 0.05}\n",
      "from state: s1, action: a1, next state: s1, propablity: 0.95, reward: 0.0\n",
      "from state: s1, action: a1, next state: s2, propablity: 0.05, reward: 0.0\n",
      "\n",
      "State: s2 action: a0 list of possible next states:  {'s0': 0.4, 's2': 0.6}\n",
      "from state: s2, action: a0, next state: s0, propablity: 0.4, reward: 0.0\n",
      "from state: s2, action: a0, next state: s2, propablity: 0.6, reward: 0.0\n",
      "\n",
      "State: s2 action: a1 list of possible next states:  {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
      "from state: s2, action: a1, next state: s0, propablity: 0.3, reward: -1\n",
      "from state: s2, action: a1, next state: s1, propablity: 0.3, reward: 0.0\n",
      "from state: s2, action: a1, next state: s2, propablity: 0.4, reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "mdp = simpleMDP()\n",
    "\n",
    "states = mdp.get_all_states()\n",
    "print(states)\n",
    "\n",
    "for s in states:\n",
    "    actions = mdp.get_possible_actions(s)\n",
    "    for a in actions:\n",
    "        next_states = mdp.get_next_states(s, a)\n",
    "        print(\"\\nState: \" + s + \" action: \" + a + \" \" + \"list of possible next states: \", next_states)\n",
    "        for next_state, propability in next_states.items():\n",
    "            reward = mdp.get_reward(s, a, next_state)\n",
    "            print(\"from state: \" + s + \", action: \" + a + \", next state: \" + next_state + \", propablity: \" + str(propability) + \", reward: \" + str(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 - Ocena strategii (*Policy evaluation*) (0.3 pkt)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu oceny strategii. Algorytm wyznacza funkcję oceny (wartości) strategii wykorzystując równanie Bellmana. Odbywa się to w sposób iteracyjny zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "\t        V_{k + 1}(s) = \\sum_a \\pi(a | s) \\sum_{s'} \\sum_r p(s', r|s, a)[r + \\gamma V_k(s')]\n",
    "\\end{equation}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem jest zainicjalizowanie strategii, która będzie podlegała ocenie. W tym zadaniu ocenie będzie podlegała strategia stochastyczna, w której będzie prawdopodobieństwo wyboru każdej z możliwych opcji będzie takie samo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': {'a0': 0.5, 'a1': 0.5}, 's1': {'a0': 0.5, 'a1': 0.5}, 's2': {'a0': 0.5, 'a1': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "policy = dict()\n",
    "\n",
    "for s in states:\n",
    "    actions = mdp.get_possible_actions(s)\n",
    "    action_prob = 1 / len(actions)\n",
    "    policy[s] = dict()\n",
    "    for a in actions:\n",
    "        policy[s][a] = action_prob\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementacja algorytmu oceny strategii - podejście z dwiema tablicami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_two_arrays(mdp, policy, gamma, theta):\n",
    "    \"\"\"\n",
    "    This function uses the two-array approach to evaluate the specified policy for the specified MDP:\n",
    "\n",
    "    'mdp' - model of the environment, use following functions:\n",
    "        get_all_states - return list of all states available in the environment\n",
    "        get_possible_actions - return list of possible actions for the given state\n",
    "        get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                          action into next_state\n",
    "        get_reward - return the reward after taking action in state and landing on next_state\n",
    "\n",
    "\n",
    "    'policy' - the stochastic policy (action probability for each state), for the given mdp, too evaluate\n",
    "    'gamma' - discount factor for MDP\n",
    "    'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is smaller\n",
    "              than theta\n",
    "    \"\"\"\n",
    "    V = dict()\n",
    "    V_prior = dict()\n",
    "\n",
    "    for s in mdp.get_all_states():\n",
    "        V[s] = 0\n",
    "        V_prior[s] = 0\n",
    "    \n",
    "        \n",
    "    #\n",
    "    # INSERT CODE HERE to evaluate the policy using the 2 array approach\n",
    "    \n",
    "    cycles = 0\n",
    "   \n",
    "    while True:\n",
    "        delta = 0\n",
    "        cycles += 1\n",
    "        \n",
    "        for s in states:\n",
    "            actions = mdp.get_possible_actions(s)\n",
    "            V_s = 0\n",
    "            \n",
    "            for a in actions:\n",
    "                transitions = mdp.get_next_states(s, a)\n",
    "                \n",
    "                for next_state, probability in transitions.items():\n",
    "                    reward = mdp.get_reward(s, a, next_state)\n",
    "                    V_s += policy[s][a] * probability * (reward + gamma * V_prior[next_state])\n",
    "            delta = max(delta, abs(V_prior[s] - V_s))\n",
    "            V[s]=V_s\n",
    "\n",
    "        V_prior = V.copy()\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    return cycles, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie poprawności zaimplementowanego algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 {'s0': 1.46785443374683, 's1': 4.5533659449117945, 's2': 1.6854414166099074}\n"
     ]
    }
   ],
   "source": [
    "cycles, V = policy_eval_two_arrays(mdp, policy, 0.9, 0.0001)\n",
    "\n",
    "\n",
    "print(cycles, V)\n",
    "assert np.isclose(V['s0'], 1.46785443374683)\n",
    "assert np.isclose(V['s1'], 4.55336594491180)\n",
    "assert np.isclose(V['s2'], 1.68544141660991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementacja algorytmu oceny strategii - obliczenia w miejscu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_in_place(mdp, policy, gamma, theta):\n",
    "    \"\"\"\n",
    "    This function uses the in-place approach to evaluate the specified policy for the specified MDP:\n",
    "\n",
    "    'mdp' - model of the environment, use following functions:\n",
    "        get_all_states - return list of all states available in the environment\n",
    "        get_possible_actions - return list of possible actions for the given state\n",
    "        get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                          action into next_state\n",
    "        get_reward - return the reward after taking action in state and landing on next_state\n",
    "\n",
    "\n",
    "    'policy' - the stochastic policy (action probability for each state), for the given mdp, too evaluate.\n",
    "    'gamma' - discount factor for MDP\n",
    "    'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is smaller\n",
    "              than theta\n",
    "    \"\"\"\n",
    "    V = dict()\n",
    "\n",
    "    for s in mdp.get_all_states():\n",
    "        V[s] = 0\n",
    "\n",
    "    # INSERT CODE HERE to evaluate the policy using the in-place approach\n",
    "    \n",
    "    cycles = 0\n",
    "   \n",
    "    while True:\n",
    "        delta = 0\n",
    "        cycles += 1\n",
    "        \n",
    "        for s in states:\n",
    "            actions = mdp.get_possible_actions(s)\n",
    "            V_s = 0\n",
    "            \n",
    "            for a in actions:\n",
    "                transitions = mdp.get_next_states(s, a)\n",
    "                \n",
    "                for next_state, probability in transitions.items():\n",
    "                    reward = mdp.get_reward(s, a, next_state)\n",
    "                    V_s += policy[s][a] * probability * (reward + gamma * V[next_state])  \n",
    "            delta = max(delta, abs(V[s] - V_s))\n",
    "            V[s]=V_s \n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    return cycles, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie poprawności zaimplementowanego algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 {'s0': 1.4681508097650982, 's1': 4.553676813271177, 's2': 1.6857723431613716}\n"
     ]
    }
   ],
   "source": [
    "cycles, V = policy_eval_in_place(mdp, policy, 0.9, 0.0001)\n",
    "\n",
    "print(cycles, V)\n",
    "\n",
    "assert np.isclose(V['s0'], 1.4681508097651)\n",
    "assert np.isclose(V['s1'], 4.5536768132712)\n",
    "assert np.isclose(V['s2'], 1.6857723431614)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 - Iteracyjne doskonalenie strategii (*Policy iteration*) (0.3 pkt)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenia jest zaimplementowanie algorytmu iteracyjnego doskonalenia strategii. Zadaniem algorytmu jest wyznaczenie optymalnej strategii dla danego środowiska. Pierwszym krokiem algorytmu jest zainicjalizowanie strategii losowymi akcjami, następnie jej ocenienie oraz poprawa, zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "    \\pi(s) \\leftarrow  \\text{argmax}_a \\sum_{s'} \\sum_r p(s', r|s, a)[r + \\gamma V(s')]\n",
    "\\end{equation}\n",
    "Jeżeli poprawiona strategia będzie się różniła od poprzedniej, należy ją ponownie ocenić i poprawić. Algorytm kończy działanie w momencie kiedy poprawiona strategia będzie dokładnie taka sama, jak ta poddawana poprawie.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj funkcję do oceny strategii, tylko tym razem przyjmij, że strategia przekazywana na wejściu będzie deteministyczna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_policy_eval_in_place(mdp, policy, gamma, theta):\n",
    "    \"\"\"\n",
    "    This function uses the in-place approach to evaluate the specified deterministic policy for the specified MDP:\n",
    "\n",
    "        'mdp' - model of the environment, use following functions:\n",
    "        get_all_states - return list of all states available in the environment\n",
    "        get_possible_actions - return list of possible actions for the given state\n",
    "        get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                          action into next_state\n",
    "        get_reward - return the reward after taking action in state and landing on next_state\n",
    "\n",
    "\n",
    "        'policy' - the deterministic policy (action probability for each state), for the given mdp, too evaluate\n",
    "        'gamma' - discount factor for MDP\n",
    "        'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is\n",
    "                  smaller than theta\n",
    "    \"\"\"\n",
    "    V = dict()\n",
    "\n",
    "    for s in mdp.get_all_states():\n",
    "        V[s] = 0\n",
    "    #\n",
    "    # INSERT CODE HERE to evaluate the deterministic policy using the in-place approach\n",
    "    \n",
    "       \n",
    "    while True:\n",
    "        delta = 0\n",
    "        \n",
    "        for s in states:\n",
    "            a = policy[s]\n",
    "            V_s = 0\n",
    "\n",
    "            transitions = mdp.get_next_states(s, a)\n",
    "            for next_state, probability in transitions.items():\n",
    "                reward = mdp.get_reward(s, a, next_state)\n",
    "                V_s += probability * (reward + gamma * V[next_state])  \n",
    "            delta = max(delta, abs(V[s] - V_s))     \n",
    "            V[s]=V_s\n",
    "            \n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj funkcję do poprawy strategii `policy` na podstawie funkcji oceny `value_function` wyznaczonej dla niej "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(mdp, policy, value_function, gamma):\n",
    "    \"\"\"\n",
    "            This function improves specified deterministic policy for the specified MDP using value_function:\n",
    "\n",
    "           'mdp' - model of the environment, use following functions:\n",
    "                get_all_states - return list of all states available in the environment\n",
    "                get_possible_actions - return list of possible actions for the given state\n",
    "                get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                                  action into next_state\n",
    "                \n",
    "                get_reward - return the reward after taking action in state and landing on next_state\n",
    "\n",
    "\n",
    "           'policy' - the deterministic policy (action for each state), for the given mdp, too improve.\n",
    "           'value_function' - the value function, for the given policy.\n",
    "            'gamma' - discount factor for MDP\n",
    "\n",
    "           Function returns False if policy was improved or True otherwise\n",
    "       \"\"\"\n",
    "\n",
    "    policy_stable = True\n",
    "\n",
    "    #\n",
    "    # INSERT CODE HERE to evaluate the improved policy\n",
    "    #\n",
    "\n",
    "    for s in states:\n",
    "        prev_action = policy[s]\n",
    "        actions = mdp.get_possible_actions(s)\n",
    "        value = 0\n",
    "        expected_rewards = []\n",
    "        for a in actions:\n",
    "            transitions = mdp.get_next_states(s, a)\n",
    "            for next_state, probability in transitions.items():\n",
    "                reward = mdp.get_reward(s, a, next_state)\n",
    "                value += probability * (reward + gamma * value_function[next_state]) \n",
    "            expected_rewards.append(value)\n",
    "            value = 0\n",
    "        policy[s] = actions[expected_rewards.index(max(expected_rewards))]\n",
    "        if prev_action != policy[s]:\n",
    "            policy_stable = False\n",
    "    \n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj funkcję do iteracyjnego doskonalenia strategii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, gamma, theta):\n",
    "\n",
    "    \"\"\"\n",
    "            This function calculate optimal policy for the specified MDP:\n",
    "\n",
    "           'mdp' - model of the environment, use following functions:\n",
    "                get_all_states - return list of all states available in the environment\n",
    "                get_possible_actions - return list of possible actions for the given state\n",
    "                get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                                  action into next_state\n",
    "                \n",
    "                get_reward - return the reward after taking action in state and landing on next_state\n",
    "\n",
    "\n",
    "           'gamma' - discount factor for MDP\n",
    "           'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is smaller\n",
    "                      than theta\n",
    "           Function returns optimal policy and value function for the policy\n",
    "       \"\"\"\n",
    "\n",
    "    policy = dict()\n",
    "\n",
    "    for s in states:\n",
    "        actions = mdp.get_possible_actions(s)\n",
    "        policy[s] = actions[0]\n",
    "\n",
    "    V = deterministic_policy_eval_in_place(mdp, policy, gamma, theta)\n",
    "\n",
    "    policy_stable = False\n",
    "\n",
    "    while not policy_stable:\n",
    "        policy_stable = policy_improvement(mdp, policy, V, gamma)\n",
    "        V = deterministic_policy_eval_in_place(mdp, policy, gamma, theta)\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie poprawności zaimplementowanego algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 'a1', 's1': 'a0', 's2': 'a1'}\n",
      "{'s0': 3.785366128142998, 's1': 7.298653645273432, 's2': 4.206831790079636}\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_value = policy_iteration(mdp, 0.9, 0.001)\n",
    "\n",
    "print(optimal_policy)\n",
    "print(optimal_value)\n",
    "\n",
    "assert optimal_policy['s0'] == 'a1'\n",
    "assert optimal_policy['s1'] == 'a0'\n",
    "assert optimal_policy['s2'] == 'a1'\n",
    "\n",
    "assert np.isclose(optimal_value['s0'], 3.78536612814300)\n",
    "assert np.isclose(optimal_value['s1'], 7.29865364527343)\n",
    "assert np.isclose(optimal_value['s2'], 4.20683179007964)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 - Iteracyjne obliczanie funkcji wartości (*Value iteration*) (0.4 pkt)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenia jest zaimplementowanie algorytmu iteracyjnego obliczania funkcji wartości. Algorytm ten łączy w sobie dwa wyżej wspomniane podejścia oceny i wyznaczania optymalnej strategii. Najpierw wyznaczana jest optymalna funkcja wartości stanu zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "    V(s) \\leftarrow  \\max_a \\sum_{s'} \\sum_r p(s', r|s, a)[r + \\gamma V(s')].\n",
    "\\end{equation}\n",
    "Po wyznaczeniu optymalnej funkcji wartości dla każdego stanu określana jest strategia postępowania w każdym możliwym stanie zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "    \\pi(s) = \\text{argmax}_a \\sum_{s'} \\sum_r p(s', r|s, a)[r + \\gamma V(s')].\n",
    "\\end{equation}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementacja algorytmu do iteracyjnego obliczania funkcji wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, gamma, theta):\n",
    "    \"\"\"\n",
    "            This function calculate optimal policy for the specified MDP using Value Iteration approach:\n",
    "\n",
    "            'mdp' - model of the environment, use following functions:\n",
    "                get_all_states - return list of all states available in the environment\n",
    "                get_possible_actions - return list of possible actions for the given state\n",
    "                get_next_states - return list of possible next states with a probability for transition from state by taking\n",
    "                                  action into next_state\n",
    "                get_reward - return the reward after taking action in state and landing on next_state\n",
    "\n",
    "\n",
    "            'gamma' - discount factor for MDP\n",
    "            'theta' - algorithm should stop when minimal difference between previous evaluation of policy and current is\n",
    "                      smaller than theta\n",
    "            Function returns optimal policy and value function for the policy\n",
    "       \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    V = dict()\n",
    "    policy = dict()\n",
    "\n",
    "    # init with a policy with first avail action for each state\n",
    "    for current_state in mdp.get_all_states():\n",
    "        V[current_state] = 0\n",
    "        actions = mdp.get_possible_actions(current_state)\n",
    "        policy[current_state] = actions[0]\n",
    "\n",
    "    #\n",
    "    # INSERT CODE HERE to evaluate the best policy and value function for the given mdp\n",
    "    #\n",
    "    \n",
    "    # policy_evaluation z algorytmem zachłannym\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            V_prev = V[s]\n",
    "            V_s = 0\n",
    "            value = 0\n",
    "            expected_rewards = []\n",
    "            actions = mdp.get_possible_actions(s)\n",
    "            for a in actions:\n",
    "                transitions = mdp.get_next_states(s, a)\n",
    "                for next_state, probability in transitions.items():\n",
    "                    reward = mdp.get_reward(s, a, next_state)\n",
    "                    value += probability * (reward + gamma * V[next_state]) \n",
    "                expected_rewards.append(value)\n",
    "                value = 0            \n",
    "            V[s] = max(expected_rewards)\n",
    "            delta = max(delta, abs(V_prev - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # policy_improvement\n",
    "    for s in states:\n",
    "        actions = mdp.get_possible_actions(s)\n",
    "        value = 0\n",
    "        expected_rewards = []\n",
    "        for a in actions:\n",
    "            transitions = mdp.get_next_states(s, a)\n",
    "            for next_state, probability in transitions.items():\n",
    "                reward = mdp.get_reward(s, a, next_state)\n",
    "                value += probability * (reward + gamma * V[next_state]) \n",
    "            expected_rewards.append(value)\n",
    "            value = 0\n",
    "        policy[s] = actions[expected_rewards.index(max(expected_rewards))]\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie poprawności zaimplementowanego algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 3.785366128142998, 's1': 7.298653645273432, 's2': 4.206831790079636}\n",
      "{'s0': 'a1', 's1': 'a0', 's2': 'a1'}\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_value = value_iteration(mdp, 0.9, 0.001)\n",
    "\n",
    "print(optimal_value)\n",
    "print(optimal_policy)\n",
    "\n",
    "assert optimal_policy['s0'] == 'a1'\n",
    "assert optimal_policy['s1'] == 'a0'\n",
    "assert optimal_policy['s2'] == 'a1'\n",
    "\n",
    "assert np.isclose(optimal_value['s0'], 3.78536612814300)\n",
    "assert np.isclose(optimal_value['s1'], 7.29865364527343)\n",
    "assert np.isclose(optimal_value['s2'], 4.20683179007964)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "493b4ae065ec4df95b127369cbef4a9b0107c6f464361b8e8e0761924d550075"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
